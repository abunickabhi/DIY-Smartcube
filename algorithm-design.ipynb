{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Receiver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The receiver for the sound-based move tracking protocol is in charge of decoding a sequence of face turns from the transmitted audio.\n",
    "\n",
    "To do this, the receiver must go through the following steps:\n",
    "- Record the transmitted audio.\n",
    "- Measure the audible frequencies at each time step.\n",
    "- Convert each time step's audible frequencies to the cube's state at that moment.\n",
    "- Decode the applied face turns from the sequence of cube states.\n",
    "\n",
    "\n",
    "This chapter will describe the development of a software algorithm in Python that can serve as a receiver for this sound-based move tracking protocol. \n",
    "This development began with the creation of synthetic audio recordings representing the tones that would be emitted by a Rubik's Cube equipped with an ideal transmitter (Section \\ref{sec:synthetic-audio-generation}) followed by the implementation of an algorithm capable of decoding that ideal synthetic audio (Section \\ref{sec:decoding-synthetic-audio}).\n",
    "The algorithm was then made more robust by adding realistic noise to the synthetic audio to better simulate a real speedcubing environment (Section \\ref{sec:adding-realistic-noise}) followed by enhancing the previously designed algorithm to continue to decode the applied move sequence in the midst of the added noise (Section: \\ref{sec:decoding-realistic-noise})."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Audio Generation\n",
    "The first step of designing a receiver is to synthesize an audio signal representative of the output of the ideal receiver.\n",
    "\n",
    "This is done by encoding the frequency corresponding to each centerpiece state (Section \\ref{subsec:represent-audio-protocol}), creating a virtual Rubik's Cube (Section \\ref{subsec:represent-rubiks-cube}), and finally generating the audio signal from the virtual Rubik's Cube's state (Section \\ref{subsec:generate-audible-algorithm})."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing the Audio Protocol\n",
    "For the synthetic audio generator to produce a realistic signal, it needs to know which frequencies to transmit for each centerpiece state. \n",
    "For this design the frequencies listed in Table \\ref{table:centerpiece-frequencies} are converted to a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "CENTERPIECE_ROTATION_TO_FREQUENCY_MAPPINGS = {\n",
    "    \"U\": {\n",
    "        0: 800,\n",
    "        1: 900,\n",
    "        2: 1000,\n",
    "        3: 1100,\n",
    "    },\n",
    "    \"D\": {\n",
    "        0: 1300,\n",
    "        1: 1400,\n",
    "        2: 1500,\n",
    "        3: 1600,\n",
    "    },\n",
    "    \"R\": {\n",
    "        0: 1800,\n",
    "        1: 1900,\n",
    "        2: 2000,\n",
    "        3: 2100,\n",
    "    },\n",
    "    \"L\": {\n",
    "        0: 2300,\n",
    "        1: 2400,\n",
    "        2: 2500,\n",
    "        3: 2600,\n",
    "    },\n",
    "    \"F\": {\n",
    "        0: 2800,\n",
    "        1: 2900,\n",
    "        2: 3000,\n",
    "        3: 3100,\n",
    "    },\n",
    "    \"B\": {\n",
    "        0: 3300,\n",
    "        1: 3400,\n",
    "        2: 3500,\n",
    "        3: 3600,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this dictionary in place, determining the frequency to transmit for any particular centerpiece's current rotation is reduced to a simple lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_of(centerpiece: str, rotation: int) -> float:\n",
    "    return CENTERPIECE_ROTATION_TO_FREQUENCY_MAPPINGS[centerpiece][rotation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing the Rubik's Cube\n",
    "Next, the synthetic audio generator needs a representation of a Rubik's Cube on which face turns can be virtually applied, and the resulting cube state can be read out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RubiksCube:\n",
    "    \n",
    "    CLOCKWISE = 1\n",
    "    COUNTERCLOCKWISE = 3\n",
    "\n",
    "    def __init__(self):\n",
    "        self.state = { \"U\": 0, \"D\": 0, \"R\": 0, \"L\": 0, \"F\": 0, \"B\": 0 }\n",
    "    \n",
    "    def apply_move(self, move: str):\n",
    "        # Extract the face and direction from the move string.\n",
    "        face = move[0]\n",
    "        if len(move) == 1:                             # e.g. U\n",
    "            direction = RubiksCube.CLOCKWISE\n",
    "        else:                                          # e.g. U'\n",
    "            direction = RubiksCube.COUNTERCLOCKWISE\n",
    "        # Update the state to apply the move.\n",
    "        self.state[face] = (self.state[face] + direction) % 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Synthetic Audio for an Arbitrary Algorithm\n",
    "Now the synthetic audio can be generated for any valid algorithm.\n",
    "This is done using the \"tones\" library \\cite{pip-tones} created by Erik Nyquist.\n",
    "First, a separate audio track is created for each centerpiece on the cube and the frequencies representing the initial state of each centerpiece are added to its corresponding audio track.\n",
    "From there, the rest of the synthetic audio can be created by iterating through each step of the given algorithm, applying it to the virtual Rubik's Cube, and adding the frequency for each centerpiece's resulting state to its corresponding audio track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tones.mixer import Mixer  # https://pypi.org/project/tones/\n",
    "from tones import SINE_WAVE\n",
    "    \n",
    "def _create_mixer(rubiks_cube: RubiksCube) -> Mixer:\n",
    "    mixer: Mixer = Mixer(sample_rate=44100, amplitude=1)\n",
    "    # Add a separate track for each centerpiece.\n",
    "    for face, _ in rubiks_cube.state.items():\n",
    "        mixer.create_track(face, SINE_WAVE, attack=0, decay=0)\n",
    "    return mixer\n",
    "\n",
    "def _render_cube_state(mixer: Mixer, rubiks_cube: RubiksCube, tps: float):\n",
    "    for face, rotation in rubiks_cube.state.items():\n",
    "        mixer.add_tone(face, frequency_of(face, rotation), duration=1 / tps)\n",
    "\n",
    "def render_audible_alg(alg: str, wav_path: str=None, tps: float=4):\n",
    "    # Create the virtual Rubik's Cube.\n",
    "    rubiks_cube: RubiksCube = RubiksCube()\n",
    "    \n",
    "    # Create the audio mixer used to create the synthesized audio.\n",
    "    mixer = _create_mixer(rubiks_cube)\n",
    "    \n",
    "    # Add the initial cube state to the mixer.\n",
    "    _render_cube_state(mixer, rubiks_cube, tps)\n",
    "    \n",
    "    # Iterate over the moves in the algorithm, adding\n",
    "    # the cube state to the mixer after each move.\n",
    "    moves = alg.split(\" \")\n",
    "    for move in moves:\n",
    "        rubiks_cube.apply_move(move)\n",
    "        _render_cube_state(mixer, rubiks_cube, tps)\n",
    "    \n",
    "    # Save the final audio to a .wav file.\n",
    "    mixer.write_wav(wav_path if wav_path else f\"{alg}.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these functions in place, synthetic audio can be easily created for any valid Rubik's Cube algorithm. For example, generating synthetic audio that sweeps through every possible centerpiece state can be done with the following snippet of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_alg = \"U U U U D D D D R R R R L L L L F F F F B B B B\"\n",
    "demo_wav_path = \"demo_all_states.wav\"\n",
    "render_audible_alg(demo_alg, demo_wav_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding Move Sequences from Synthetic Audio\n",
    "With synthetic audio now available for any valid Rubik's Cube algorithm, the next step is to create an initial software algorithm that can decode that audio back into the original move sequence.\n",
    "\n",
    "TODO summarize the subsections that make this possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Spectrogram\n",
    "The first step in decoding the synthetic audio is determining its component frequencies at any specific moment in time.\n",
    "These component frequencies can be easily visualized using a spectrogram, like the one in Figure \\ref{fig:spectrogram} of the synthetic audio created in Section \\ref{subsec:generate-audible-algorithm}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![spectrogram](\"./thesis/draft/Figures/5 Algorithm Design/component_frequencies.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bright, horizontal yellow bands in the spectrogram represent the component frequencies present in the audio signal at each point in time.\n",
    "The vertical red line on the spectrogram indicates the specific slice of the spectrogram represented by the graph of the component frequencies at that specific point in time.\n",
    "\n",
    "While the Figure \\ref{fig:spectrogram} was created using Matplotlib's specgram plot, the actual data of the spectogram for a given .wav file can be easily obtained using the numpy and scipy packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "\n",
    "def compute_spectrogram(wav_path: str):\n",
    "    SAMPLES_PER_WINDOW = 1024  # Balances frequency/time precision\n",
    "    sample_rate, audio_samples = wavfile.read(wav_path)\n",
    "    freq, time, Zxx = signal.stft(audio_samples, fs=sample_rate,\n",
    "        nperseg=SAMPLES_PER_WINDOW, noverlap=(SAMPLES_PER_WINDOW // 4) * 3)\n",
    "    spectrogram = np.abs(Zxx).transpose()\n",
    "    return freq, time, spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the Dominant Component Frequencies\n",
    "Notice how the component frequency graph in Figure \\ref{fig:spectrogram} has six peaks: these are the transmitted frequencies representing the current state of the virtual Rubik's Cube's six centerpieces at that specific instant of time.\n",
    "\n",
    "The exact frequencies of these peaks can be extracted by filtering out all frequencies not above a specific threshold.\n",
    "In this case, a simple threshold of 85\\% of the maximum strength of any component frequency (see the green line in Figure \\ref{fig:spectrogram-with-naive-threshold}) isolates the peak of the six dominant component frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![threshold](\"./thesis/draft/Figures/5 Algorithm Design/threshold.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the actual spectogram data we can compute the exact values of these peaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['818 Hz', '1593 Hz', '1809 Hz', '2283 Hz', '2799 Hz', '3316 Hz']\n"
     ]
    }
   ],
   "source": [
    "def compute_threshold(values: list):\n",
    "    return max(values) * 0.85\n",
    "\n",
    "def extract_important_frequencies(freq, time, spectrogram, time_idx):\n",
    "    important_freqs = []\n",
    "    threshold = compute_threshold(spectrogram[time_idx])\n",
    "    for freq_idx in range(len(freq)):\n",
    "        if spectrogram[time_idx][freq_idx] > threshold:\n",
    "            important_freqs.append(dict(\n",
    "                hz=freq[freq_idx],\n",
    "                power=spectrogram[time_idx][freq_idx]\n",
    "            ))\n",
    "    return important_freqs\n",
    "\n",
    "freq, time, spectrogram = compute_spectrogram(demo_wav_path)\n",
    "important_freqs = extract_important_frequencies(freq, time, spectrogram,\n",
    "    time_idx=310) # 1.80 seconds\n",
    "\n",
    "important_freqs_hz = [i[\"hz\"] for i in important_freqs]\n",
    "pretty_freqs = list(map(lambda x: f\"{x:.0f} Hz\", important_freqs_hz))\n",
    "print(pretty_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translating Component Frequencies to Centerpiece States\n",
    "With the specific frequencies of each detected peak, the original state of the Rubik's Cube at that moment in time can be computed by finding the state whose corresponding frequency is closest to each detected peak frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'U': 0, 'D': 3, 'R': 0, 'L': 0, 'F': 0, 'B': 0}\n"
     ]
    }
   ],
   "source": [
    "def _closest_state(detected_freq):\n",
    "    closest_rotation = None\n",
    "    closest_difference = None\n",
    "    for face, rotations in CENTERPIECE_ROTATION_TO_FREQUENCY_MAPPINGS.items():\n",
    "        for rotation, freq in rotations.items():\n",
    "            difference = abs(detected_freq - freq)\n",
    "            if closest_difference is None or difference < closest_difference:\n",
    "                closest_difference = difference\n",
    "                closest_rotation = {face: rotation}\n",
    "    return closest_rotation\n",
    "\n",
    "def get_state_from_freqs(important_freqs: list) -> dict:\n",
    "    state = {}\n",
    "    for freq in important_freqs:\n",
    "        state.update(_closest_state(freq))\n",
    "    return state\n",
    "\n",
    "print(get_state_from_freqs(important_freqs_hz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating this process for each time step in the recorded audio will yield a sequence of the Rubik's Cube's centerpiece states over the course of the recorded audio sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_over_time(freq, time, spectrogram):\n",
    "    state_over_time = []\n",
    "    for time_idx in range(len(time)):\n",
    "        important_freqs = extract_important_frequencies(\n",
    "            freq, time, spectrogram, time_idx)\n",
    "        important_freqs_hz = [i[\"hz\"] for i in important_freqs]\n",
    "        state = get_state_from_freqs(important_freqs_hz)\n",
    "        state_over_time.append(dict(\n",
    "            time=time[time_idx],\n",
    "            state=state\n",
    "        ))\n",
    "    return state_over_time\n",
    "\n",
    "state_over_time = get_state_over_time(freq, time, spectrogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Move Sequences from Centerpiece State Sequences\n",
    "Finally, the original move sequence can be recovered by iterating over the sequence of cube states, and registering any change to the cube state as a move applied to the cube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U U U U D D D D R R R R L L L L F F F F B B B B\n",
      "Matches demo_alg? True\n"
     ]
    }
   ],
   "source": [
    "def _move_from(face, current_rotation, new_rotation):\n",
    "    direction = None\n",
    "    if (current_rotation + RubiksCube.CLOCKWISE) % 4 == new_rotation:\n",
    "        direction = \"\"  # Clockwise\n",
    "    elif (current_rotation + RubiksCube.COUNTERCLOCKWISE) % 4 == new_rotation:\n",
    "        direction = \"'\" # Counterclockwise\n",
    "    return f\"{face}{direction}\"\n",
    "\n",
    "def detect_moves(state_over_time):\n",
    "    detected_moves = []\n",
    "    current_state = state_over_time[0][\"state\"]\n",
    "    for timed_state in state_over_time:\n",
    "        time, state = timed_state.values()\n",
    "        for face, rotation in state.items():\n",
    "            if rotation != current_state[face]:\n",
    "                detected_moves.append(dict(\n",
    "                    time=time,\n",
    "                    move=_move_from(face, current_state[face], rotation)\n",
    "                ))\n",
    "                current_state[face] = rotation\n",
    "    return detected_moves\n",
    "\n",
    "detected_moves = detect_moves(state_over_time)\n",
    "\n",
    "pretty_moves = \" \".join([i[\"move\"] for i in detected_moves])\n",
    "print(pretty_moves)   \n",
    "print(f\"Matches demo_alg? {demo_alg == pretty_moves}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the detected move sequence matches the demo algorithm used to create the synthetic audio in Section \\ref{subsec:generate-audible-algorithm}, which means this algorithm is a functional receiver for an ideal transmitter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Side Note: Here's the full implementation for a second example. This is provided here so it's easy to find a complete example for detecting a new algorithm even though it's not included in the main Thesis document.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U U' D D' R R' L L' F F' B B'\n",
      "Matches demo2_alg? True\n"
     ]
    }
   ],
   "source": [
    "demo2_alg = \"U U' D D' R R' L L' F F' B B'\"\n",
    "demo2_wav_path = \"demo_all_turns.wav\"\n",
    "render_audible_alg(demo2_alg, demo2_wav_path)\n",
    "\n",
    "freq2, time2, spectrogram2 = compute_spectrogram(demo2_wav_path)\n",
    "state_over_time2 = get_state_over_time(freq2, time2, spectrogram2)\n",
    "detected_moves2 = detect_moves(state_over_time2)\n",
    "\n",
    "pretty_moves2 = \" \".join([i[\"move\"] for i in detected_moves2])\n",
    "print(pretty_moves2)\n",
    "print(f\"Matches demo2_alg? {demo2_alg == pretty_moves2}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9bb1448f44d41b50ef724d977467d677366b56f1ac5b7ddcacc005f02a40cb4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
